{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09199e-c5d4-431e-8982-26e7d17437bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#尝试使用复数卷积\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Conv1d\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import thop\n",
    "from torchinfo import summary\n",
    "import time\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = 1000\n",
    "set_seed(seed)\n",
    "use_cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device('cuda:0') if use_cuda else torch.device('cpu')\n",
    "print(device)\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "class RealToComplex(nn.Module):\n",
    "    \"\"\"可训练的实数到复数转换层\"\"\"\n",
    "    def __init__(self, seq_len):\n",
    "        super().__init__()\n",
    "        # 使用两个独立的线性层分别生成实部和虚部\n",
    "        self.real_proj = nn.Linear(seq_len, seq_len)\n",
    "        self.imag_proj = nn.Linear(seq_len, seq_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x 形状: [batch, 1, seq_len]\n",
    "        x = x.squeeze(1)  # [batch, seq_len]\n",
    "        real = self.real_proj(x)\n",
    "        imag = self.imag_proj(x)\n",
    "        return torch.complex(real, imag).unsqueeze(1)  # [batch, 1, seq_len] complex\n",
    "\n",
    "class ComplexEncoder(nn.Module):\n",
    "    def __init__(self, N, G):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.G = G\n",
    "        self.seq_len = 128  \n",
    "        \n",
    "        assert 256 % G == 0, \"256 must be divisible by G\"\n",
    "        assert 64 % G == 0, \"64 must be divisible by G\"\n",
    "        assert 32 % G == 0, \"32 must be divisible by G\"\n",
    "\n",
    "        # 可训练的实数到复数转换器\n",
    "        self.real_to_complex = RealToComplex(self.seq_len)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            ComplexConv1d(1, 256, 5, 1, 2),\n",
    "            ComplexReLU(),\n",
    "            ComplexConv1d(256, 64, 3, 1, 1, groups=G),\n",
    "            ComplexReLU(),\n",
    "            ComplexConv1d(64, 32, 3, 1, 1, groups=G),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = ComplexConv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.timedis1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            TimeDistributed(nn.Linear(64, 2), True),\n",
    "            nn.BatchNorm1d(N),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.real_to_complex(x)\n",
    "        # 通过复数卷积层\n",
    "        out1 = self.encoder(x)\n",
    "        out1_ori = self.shortcut(x)\n",
    "        combined = out1 + out1_ori\n",
    "\n",
    "        # 将复数转换为实数（拼接实部和虚部）\n",
    "        real_part = combined.real\n",
    "        imag_part = combined.imag\n",
    "        tx_input = torch.cat([real_part, imag_part], dim=1)\n",
    "\n",
    "        # 通过时间分布层\n",
    "        tx = self.timedis1(\n",
    "            tx_input.reshape(x.shape[0], 64, -1).transpose(1, 2)\n",
    "        ).reshape(x.shape[0], 2, -1)\n",
    "\n",
    "        # 功率归一化\n",
    "        power_per_timestep = tx.pow(2).sum(dim=1)\n",
    "        avg_power = power_per_timestep.mean(dim=1, keepdim=True)\n",
    "        scale_factor = 1.0 / torch.sqrt(avg_power + 1e-8)\n",
    "        tx_normalized = tx * scale_factor.unsqueeze(-1)\n",
    "\n",
    "        return tx_normalized\n",
    "\n",
    "def apply_complex_1d(fr, fi, x):\n",
    "    real = fr(x.real) - fi(x.imag)\n",
    "    imag = fr(x.imag) + fi(x.real)\n",
    "    return torch.complex(real, imag)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first \n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  #(batch_size * 128 , 40)\n",
    "        y = self.module(x_reshape) #(batch_size * 128 , 1)\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  #(batch_size, 128, 1)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))\n",
    "        return y  \n",
    "\n",
    "class ComplexReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.complex(F.relu(x.real), F.relu(x.imag))\n",
    "    \n",
    "class ComplexConv1d(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(ComplexConv1d, self).__init__()\n",
    "        assert in_channels % groups == 0, 'in_channels must be divisible by groups'\n",
    "        assert out_channels % groups == 0, 'out_channels must be divisible by groups'\n",
    "        \n",
    "        self.conv_r = Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            dilation,\n",
    "            groups,\n",
    "            bias,\n",
    "        )\n",
    "        self.conv_i = Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            dilation,\n",
    "            groups,\n",
    "            bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return apply_complex_1d(self.conv_r, self.conv_i, inp)\n",
    "\n",
    "class NaiveComplexBatchNorm1d(Module):\n",
    "    \"\"\"\n",
    "    Naive approach to complex batch norm, perform batch norm independently on real and imaginary part.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        eps=1e-5,\n",
    "        momentum=0.1,\n",
    "        affine=True,\n",
    "        track_running_stats=True,\n",
    "    ):\n",
    "        super(NaiveComplexBatchNorm1d, self).__init__()\n",
    "        self.bn_r = nn.BatchNorm1d(\n",
    "            num_features, eps, momentum, affine, track_running_stats\n",
    "        )\n",
    "        self.bn_i = nn.BatchNorm1d(\n",
    "            num_features, eps, momentum, affine, track_running_stats\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return torch.complex(self.bn_r(inp.real), self.bn_i(inp.imag))\n",
    "\n",
    "class ComplexMultiScaleConvBlock(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super().__init__()\n",
    "        self.conv3x1 = ComplexConv1d(1, K, kernel_size=3, padding=1)\n",
    "        self.conv5x1 = ComplexConv1d(1, K, kernel_size=5, padding=2)\n",
    "        self.conv7x1 = ComplexConv1d(1, K, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.bn3 = NaiveComplexBatchNorm1d(K)\n",
    "        self.bn5 = NaiveComplexBatchNorm1d(K)\n",
    "        self.bn7 = NaiveComplexBatchNorm1d(K)\n",
    "        \n",
    "        self.relu = ComplexReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3 = self.relu(self.bn3(self.conv3x1(x)))\n",
    "        branch5 = self.relu(self.bn5(self.conv5x1(x)))\n",
    "        branch7 = self.relu(self.bn7(self.conv7x1(x)))\n",
    "        return torch.cat([branch3, branch5, branch7], dim=1)\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, Cin):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        # 压缩层：输入Cin*2（实部+虚部），输出Cin//r\n",
    "        self.compress = nn.Conv1d(Cin*2, Cin, 1)  \n",
    "        self.excitation = nn.Conv1d(Cin, Cin*2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 拼接实部和虚部 [batch, 2*K, length]\n",
    "        x_split = torch.cat([x.real, x.imag], dim=1)\n",
    "        # 压缩和激励\n",
    "        out = self.squeeze(x_split)          # [batch, 2*K, 1]\n",
    "        out = self.relu(self.compress(out))  # [batch, K, 1]\n",
    "        out = self.sigmoid(self.excitation(out))  # [batch, 2*K, 1]\n",
    "        # 分离实虚部权重\n",
    "        out_real, out_imag = torch.chunk(out, 2, dim=1)\n",
    "        # 复数加权\n",
    "        return torch.complex(x.real * out_real, x.imag * out_imag)\n",
    "\n",
    "class PreAttn(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(PreAttn, self).__init__()\n",
    "        self.K = K\n",
    "        \n",
    "        self.complex_multiscale = ComplexMultiScaleConvBlock(K//2)  # K//2个复数通道\n",
    "        self.complex_conv1x1 = ComplexConv1d(3*(K//2), K//2, kernel_size=1)\n",
    "        self.bn = NaiveComplexBatchNorm1d(K//2)\n",
    "        self.relu = ComplexReLU()\n",
    "        self.shortcut = ComplexConv1d(1, K//2, kernel_size=1)\n",
    "        self.attention = SE(K//2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 转换为复数\n",
    "        x_permuted = x.permute(0, 2, 1)  # [batch, length, 2]\n",
    "        x_complex = torch.view_as_complex(x_permuted.contiguous())  # [batch, length]\n",
    "        x_complex = x_complex.unsqueeze(1)  # [batch, 1, length]\n",
    "        \n",
    "        # 主路径\n",
    "        out1 = self.complex_multiscale(x_complex)\n",
    "        out1 = self.complex_conv1x1(out1)\n",
    "        out1 = self.relu(self.bn(out1))\n",
    "        \n",
    "        # 注意力\n",
    "        out2 = self.attention(out1)\n",
    "        \n",
    "        # 捷径路径\n",
    "        out3 = self.shortcut(x_complex)\n",
    "        \n",
    "        # 残差连接\n",
    "        complex_out = out2 + out3\n",
    "        \n",
    "        # 转换为实数\n",
    "        real_out = torch.cat([complex_out.real, complex_out.imag], dim=1)\n",
    "        \n",
    "        return real_out\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first \n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  #(batch_size * 128 , 40)\n",
    "        y = self.module(x_reshape) #(batch_size * 128 , 1)\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  #(batch_size, 128, 1)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))\n",
    "        return y\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_experts = 28  # 专家数量\n",
    "        self.top_k = 12         # 选择top专家\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(self.num_experts)\n",
    "        ])\n",
    "        self.gate = nn.Linear(dim, self.num_experts)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d = x.shape\n",
    "        x_flat = x.reshape(-1, d)  # (batch*seq_len, d)\n",
    "\n",
    "        logits = self.gate(x_flat)  # (batch*seq_len, num_experts)\n",
    "        topk_logits, topk_indices = logits.topk(k=self.top_k, dim=-1)  \n",
    "        topk_gates = torch.softmax(topk_logits, dim=-1)               \n",
    "        topk_gates = self.dropout(topk_gates)\n",
    "        \n",
    "        x_repeated = x_flat.repeat_interleave(self.top_k, dim=0)      # (batch*seq_len*6, d)\n",
    "        expert_indices = topk_indices.flatten()                       # (batch*seq_len*6)\n",
    "\n",
    "        # 初始化输出并聚合专家结果\n",
    "        out_flat = torch.zeros_like(x_flat)\n",
    "        for expert_id in range(self.num_experts):\n",
    "            mask = expert_indices == expert_id\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            \n",
    "            expert_input = x_repeated[mask]\n",
    "            expert_output = self.experts[expert_id](expert_input)\n",
    "            \n",
    "            # 计算加权输出并映射回原始位置\n",
    "            original_indices = torch.arange(batch_size * seq_len, device=x.device)\n",
    "            original_indices = original_indices.repeat_interleave(self.top_k)[mask]\n",
    "            weighted_output = expert_output * topk_gates.flatten()[mask].unsqueeze(1)\n",
    "            out_flat.index_add_(0, original_indices, weighted_output)\n",
    "\n",
    "        return out_flat.reshape(batch_size, seq_len, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8caa68-4f8e-4ab3-b72e-b547d3483364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = heads*dim_head\n",
    "        project_out = not (heads == 1 and dim_head == dim) \n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1) \n",
    "        # b:batch_size  n:channel  h:heads  d:dim_head\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv) \n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale \n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v) \n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class BinaryVoVUnit(nn.Module):\n",
    "    \"\"\"\n",
    "    在模块末尾通过卷积将长度 L 从 135 降到 128：\n",
    "    - 主分支：Conv1d(k=8, s=1, p=0) -> 128\n",
    "    - 残差分支：AvgPool1d(k=8, s=1) 同样降到 128；必要时再用 1x1 Conv 对齐通道\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, branch_channels, num_branches, use_residual):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "        # 分支\n",
    "        self.branches = nn.ModuleList()\n",
    "        ch_in = in_channels\n",
    "        for _ in range(num_branches):\n",
    "            self.branches.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(ch_in, branch_channels, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm1d(branch_channels),\n",
    "                    nn.ReLU(inplace=False)\n",
    "                )\n",
    "            )\n",
    "            ch_in = branch_channels\n",
    "\n",
    "        # 拼接 + 压缩\n",
    "        self.concat_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels + num_branches * branch_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "        # —— 核心：长度压缩到 128（135 -> 128）\n",
    "        self.len_reduce = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=8, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "        # 残差分支的长度对齐与通道对齐\n",
    "        if use_residual:\n",
    "            # 仅长度对齐：使用平均池化（不改变通道数）\n",
    "            self.res_len = nn.AvgPool1d(kernel_size=8, stride=1)  # L_out = L_in - 7\n",
    "            # 通道不一致时的投影\n",
    "            self.res_proj = None\n",
    "            if in_channels != out_channels:\n",
    "                self.res_proj = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        outs = [x]\n",
    "        for br in self.branches:\n",
    "            x = br(x)\n",
    "            outs.append(x)\n",
    "\n",
    "        x = torch.cat(outs, dim=1)\n",
    "        x = self.concat_conv(x)          # [N, C_out, 135]（示例）\n",
    "        x = self.len_reduce(x)           # -> [N, C_out, 128]\n",
    "\n",
    "        if self.use_residual:\n",
    "            identity = self.res_len(identity)  # [N, C_in, 128]\n",
    "            if self.res_proj is not None:\n",
    "                identity = self.res_proj(identity)  # [N, C_out, 128]\n",
    "            x = x + identity\n",
    "\n",
    "        return x\n",
    "\n",
    "       \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, N, G, K):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.G = G\n",
    "        self.K = K\n",
    "        #[batch_size, 40, 128]\n",
    "        self.decoder = Transformer(dim=N, depth=2, heads=8, dim_head=32, mlp_dim=N* 2, dropout=0.05) #[batch_size, 40, 128]\n",
    "\n",
    "        #[batch_size, 128, 40]\n",
    "        self.timedis2 = nn.Sequential(\n",
    "            TimeDistributed(nn.Linear(K, 1), batch_first=True),\n",
    "            nn.Sigmoid(),\n",
    "        ) #[batch_size, 128, 1]\n",
    "\n",
    "    #[batch_size, 40, 128]\n",
    "    def forward(self, R):\n",
    "        out = self.decoder(R).reshape(R.shape[0], self.K, -1).transpose(1, 2).contiguous() #[batch_size, 128, 40]\n",
    "        out = self.timedis2(out).reshape(R.shape[0], 1, -1)#[batch_size, 1, 128]\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    for SISO frequency-selective\n",
    "    input: 128 × 1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.transmitter = ComplexEncoder(128, 4)\n",
    "        self.receiver = nn.Sequential(\n",
    "            # nn.Conv1d(80, 40, kernel_size=1),\n",
    "            BinaryVoVUnit(in_channels=70, out_channels=35, branch_channels=5, num_branches=3, use_residual=True),\n",
    "            Decoder(128, 2, 35),\n",
    "            )\n",
    "        self.feature_extractor = PreAttn(70)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def channel(self, input_signals, SNR, H_r, H_i, batch_size):\n",
    "        #[2, batch_size, 128]\n",
    "        assert input_signals.dim() == 3\n",
    "        real = input_signals[0]\n",
    "        imag = input_signals[1]\n",
    "        S = torch.mean(real ** 2 + imag ** 2)\n",
    "        snr_linear = 10 ** (SNR / 10.0)\n",
    "        noise_variance = S / (2 * snr_linear)\n",
    "\n",
    "        h_r = torch.zeros(batch_size, H_r.shape[-1]).to(device)\n",
    "        h_r = H_r\n",
    "        h_i = torch.zeros(batch_size, H_i.shape[-1]).to(device)\n",
    "        h_i = H_i\n",
    "\n",
    "        def conv(x, h):\n",
    "            # x:[batch_size, 128]\n",
    "            # h:[batch_size, 8]\n",
    "            h = h.flip(dims=[-1]).reshape(batch_size, 1, -1)  # [batch_size, 1, 8]\n",
    "            # x:[1, batch_size, 128] h:[batch_size, 1, 8]\n",
    "            y = F.conv1d(x.reshape(1, x.shape[0], -1), h, padding=(H_i.shape[-1] - 1), groups=x.shape[0])\n",
    "            # y:[1, batch_size, 135]\n",
    "            y = y.reshape(input_signals.shape[1], -1)  # y:[batch_size, 135]  —— 去掉原来的 [:, :x.shape[1]] 裁剪\n",
    "            return y  # [batch_size, 135]\n",
    "\n",
    "\n",
    "        out_r = conv(real, h_r) - conv(imag, h_i) \n",
    "        out_i = conv(imag, h_r) + conv(real, h_i)\n",
    "\n",
    "        out = torch.stack((out_r, out_i), dim=0)\n",
    "        noise = torch.sqrt(noise_variance) * torch.randn_like(out, device=device)\n",
    "        out += noise\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, snr, h_r, h_i, batch_size):\n",
    "        #[batchsize,1,128]\n",
    "        tx = self.transmitter(x)#[batch_size, 2, 128]\n",
    "        xx = torch.cat((tx[:, 0, :], tx[:, 1, :])).reshape(2, x.shape[0], -1)#[2, batch_size, 128]\n",
    "        rx = self.channel(xx, snr, h_r, h_i, batch_size)#[2, batch_size, 128]\n",
    "        r = torch.cat((rx[0], rx[1]), 1).reshape(x.shape[0], 2, -1)#[batch_size, 2, 128]\n",
    "        z1 = self.feature_extractor(r)\n",
    "        #R = torch.einsum(\"bcl,bz->bczl\", r, z1).view(r.shape[0], -1, r.shape[-1])\n",
    "        out = self.receiver(z1)#[batch_size, 1, 128]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f27afa-cc2a-43ab-97c1-ca73ba1c841e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre trained model found. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u2021213363/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.60060253, Val loss: 0.46909063, Val BER: 0.233184408, Time: 68.5s, Current: 00:04, ETA: 04:49\n",
      "Initial Learning Rate: 1.00e-03\n",
      "Epoch: 1, Train loss: 0.32404624, Val loss: 0.19494186, Val BER: 0.082079590, Time: 67.1s, Current: 00:06, ETA: 04:46\n",
      "Epoch: 2, Train loss: 0.11422776, Val loss: 0.04642085, Val BER: 0.016456641, Time: 67.1s, Current: 00:07, ETA: 04:45\n",
      "Epoch: 3, Train loss: 0.03835401, Val loss: 0.02097795, Val BER: 0.007225033, Time: 67.2s, Current: 00:08, ETA: 04:45\n",
      "Epoch: 4, Train loss: 0.02263247, Val loss: 0.01354838, Val BER: 0.004598047, Time: 67.8s, Current: 00:09, ETA: 04:46\n",
      "Epoch: 5, Train loss: 0.01585298, Val loss: 0.00924480, Val BER: 0.003113151, Time: 68.1s, Current: 00:10, ETA: 04:46\n",
      "Epoch: 6, Train loss: 0.01207658, Val loss: 0.00698577, Val BER: 0.002344727, Time: 68.2s, Current: 00:11, ETA: 04:47\n",
      "Epoch: 7, Train loss: 0.00948801, Val loss: 0.00537842, Val BER: 0.001794010, Time: 68.1s, Current: 00:12, ETA: 04:47\n",
      "Epoch: 8, Train loss: 0.00761523, Val loss: 0.00434316, Val BER: 0.001440137, Time: 68.0s, Current: 00:13, ETA: 04:47\n",
      "Epoch: 9, Train loss: 0.00635036, Val loss: 0.00352140, Val BER: 0.001164193, Time: 68.4s, Current: 00:15, ETA: 04:47\n",
      "Epoch: 10, Train loss: 0.00550512, Val loss: 0.00282862, Val BER: 0.000931868, Time: 68.2s, Current: 00:16, ETA: 04:48\n",
      "Epoch: 11, Train loss: 0.00466707, Val loss: 0.00253682, Val BER: 0.000835840, Time: 68.2s, Current: 00:17, ETA: 04:48\n",
      "Epoch: 12, Train loss: 0.00418284, Val loss: 0.00229796, Val BER: 0.000763607, Time: 68.0s, Current: 00:18, ETA: 04:48\n",
      "Epoch: 13, Train loss: 0.00384667, Val loss: 0.00199176, Val BER: 0.000646712, Time: 68.2s, Current: 00:19, ETA: 04:48\n",
      "Epoch: 14, Train loss: 0.00346489, Val loss: 0.00188990, Val BER: 0.000621289, Time: 68.2s, Current: 00:20, ETA: 04:48\n",
      "Epoch: 15, Train loss: 0.00318169, Val loss: 0.00169606, Val BER: 0.000548926, Time: 68.3s, Current: 00:21, ETA: 04:48\n",
      "Epoch: 16, Train loss: 0.00295339, Val loss: 0.00158053, Val BER: 0.000519010, Time: 68.3s, Current: 00:23, ETA: 04:48\n",
      "Epoch: 17, Train loss: 0.00270445, Val loss: 0.00134428, Val BER: 0.000439355, Time: 68.5s, Current: 00:24, ETA: 04:48\n",
      "Epoch: 18, Train loss: 0.00253923, Val loss: 0.00133920, Val BER: 0.000437598, Time: 68.4s, Current: 00:25, ETA: 04:48\n",
      "Epoch: 19, Train loss: 0.00233398, Val loss: 0.00112457, Val BER: 0.000364160, Time: 68.4s, Current: 00:26, ETA: 04:48\n",
      "Epoch: 20, Train loss: 0.00216177, Val loss: 0.00107888, Val BER: 0.000351921, Time: 68.2s, Current: 00:27, ETA: 04:48\n",
      "Epoch: 21, Train loss: 0.00205909, Val loss: 0.00105422, Val BER: 0.000343457, Time: 68.4s, Current: 00:28, ETA: 04:48\n",
      "Epoch: 22, Train loss: 0.00199324, Val loss: 0.00096940, Val BER: 0.000318815, Time: 68.4s, Current: 00:29, ETA: 04:48\n",
      "Epoch: 23, Train loss: 0.00180156, Val loss: 0.00101160, Val BER: 0.000328125, Time: 68.2s, Current: 00:31, ETA: 04:48\n",
      "Epoch: 24, Train loss: 0.00172119, Val loss: 0.00088554, Val BER: 0.000288477, Time: 68.4s, Current: 00:32, ETA: 04:48\n",
      "Epoch: 25, Train loss: 0.00164302, Val loss: 0.00081803, Val BER: 0.000264811, Time: 68.3s, Current: 00:33, ETA: 04:49\n",
      "Epoch: 26, Train loss: 0.00154155, Val loss: 0.00075065, Val BER: 0.000238281, Time: 68.4s, Current: 00:34, ETA: 04:49\n",
      "Epoch: 27, Train loss: 0.00152398, Val loss: 0.00071065, Val BER: 0.000229264, Time: 68.3s, Current: 00:35, ETA: 04:49\n",
      "Epoch: 28, Train loss: 0.00139280, Val loss: 0.00067197, Val BER: 0.000216992, Time: 68.3s, Current: 00:36, ETA: 04:49\n",
      "Epoch: 29, Train loss: 0.00136877, Val loss: 0.00073088, Val BER: 0.000240169, Time: 68.3s, Current: 00:37, ETA: 04:49\n",
      "Epoch: 30, Train loss: 0.00132988, Val loss: 0.00063328, Val BER: 0.000202441, Time: 68.3s, Current: 00:39, ETA: 04:49\n",
      "Epoch: 31, Train loss: 0.00125161, Val loss: 0.00059119, Val BER: 0.000190755, Time: 68.5s, Current: 00:40, ETA: 04:49\n",
      "Epoch: 32, Train loss: 0.00119076, Val loss: 0.00055014, Val BER: 0.000176725, Time: 68.3s, Current: 00:41, ETA: 04:49\n",
      "Epoch: 33, Train loss: 0.00115875, Val loss: 0.00055577, Val BER: 0.000179850, Time: 68.3s, Current: 00:42, ETA: 04:49\n",
      "Epoch: 34, Train loss: 0.00112571, Val loss: 0.00055276, Val BER: 0.000180924, Time: 68.1s, Current: 00:43, ETA: 04:49\n",
      "Epoch: 35, Train loss: 0.00106172, Val loss: 0.00051308, Val BER: 0.000163444, Time: 68.0s, Current: 00:44, ETA: 04:49\n",
      "Epoch: 36, Train loss: 0.00106588, Val loss: 0.00053391, Val BER: 0.000171680, Time: 67.9s, Current: 00:45, ETA: 04:49\n",
      "Epoch: 37, Train loss: 0.00102336, Val loss: 0.00047884, Val BER: 0.000153418, Time: 67.8s, Current: 00:47, ETA: 04:49\n",
      "Epoch: 38, Train loss: 0.00095380, Val loss: 0.00049707, Val BER: 0.000160254, Time: 67.6s, Current: 00:48, ETA: 04:49\n",
      "Epoch: 39, Train loss: 0.00091862, Val loss: 0.00043961, Val BER: 0.000142253, Time: 67.3s, Current: 00:49, ETA: 04:48\n",
      "Epoch: 40, Train loss: 0.00093741, Val loss: 0.00045666, Val BER: 0.000145085, Time: 67.3s, Current: 00:50, ETA: 04:48\n",
      "Epoch: 41, Train loss: 0.00089741, Val loss: 0.00044333, Val BER: 0.000142546, Time: 67.7s, Current: 00:51, ETA: 04:48\n",
      "Epoch: 42, Train loss: 0.00084728, Val loss: 0.00041204, Val BER: 0.000130632, Time: 67.3s, Current: 00:52, ETA: 04:48\n",
      "Epoch: 43, Train loss: 0.00085112, Val loss: 0.00038199, Val BER: 0.000122493, Time: 67.2s, Current: 00:53, ETA: 04:48\n",
      "Epoch: 44, Train loss: 0.00082347, Val loss: 0.00043814, Val BER: 0.000141992, Time: 67.3s, Current: 00:55, ETA: 04:48\n",
      "Epoch: 45, Train loss: 0.00078055, Val loss: 0.00039902, Val BER: 0.000128158, Time: 67.4s, Current: 00:56, ETA: 04:48\n",
      "Epoch: 46, Train loss: 0.00077995, Val loss: 0.00037627, Val BER: 0.000119727, Time: 67.6s, Current: 00:57, ETA: 04:48\n",
      "Epoch: 47, Train loss: 0.00076418, Val loss: 0.00041268, Val BER: 0.000131999, Time: 67.3s, Current: 00:58, ETA: 04:48\n",
      "Epoch: 48, Train loss: 0.00073828, Val loss: 0.00030300, Val BER: 0.000096126, Time: 67.1s, Current: 00:59, ETA: 04:48\n",
      "Epoch: 49, Train loss: 0.00071565, Val loss: 0.00032120, Val BER: 0.000103418, Time: 67.2s, Current: 01:00, ETA: 04:48\n",
      "Epoch: 50, Train loss: 0.00069009, Val loss: 0.00033346, Val BER: 0.000107454, Time: 67.2s, Current: 01:01, ETA: 04:48\n",
      "Epoch: 51, Train loss: 0.00072641, Val loss: 0.00033455, Val BER: 0.000108496, Time: 67.5s, Current: 01:02, ETA: 04:48\n",
      "Epoch: 52, Train loss: 0.00068417, Val loss: 0.00037225, Val BER: 0.000118457, Time: 67.3s, Current: 01:04, ETA: 04:48\n",
      "Epoch: 53, Train loss: 0.00064637, Val loss: 0.00028538, Val BER: 0.000091927, Time: 67.4s, Current: 01:05, ETA: 04:48\n",
      "Epoch: 54, Train loss: 0.00065913, Val loss: 0.00031953, Val BER: 0.000100488, Time: 67.1s, Current: 01:06, ETA: 04:47\n",
      "Epoch: 55, Train loss: 0.00062337, Val loss: 0.00029586, Val BER: 0.000093620, Time: 67.4s, Current: 01:07, ETA: 04:47\n",
      "Epoch: 56, Train loss: 0.00065286, Val loss: 0.00032403, Val BER: 0.000102897, Time: 67.3s, Current: 01:08, ETA: 04:47\n",
      "Epoch: 57, Train loss: 0.00062004, Val loss: 0.00026055, Val BER: 0.000082910, Time: 67.1s, Current: 01:09, ETA: 04:47\n",
      "Epoch: 58, Train loss: 0.00059923, Val loss: 0.00027270, Val BER: 0.000089193, Time: 67.6s, Current: 01:10, ETA: 04:47\n",
      "Epoch: 59, Train loss: 0.00058542, Val loss: 0.00029103, Val BER: 0.000092513, Time: 67.3s, Current: 01:11, ETA: 04:47\n",
      "Epoch: 60, Train loss: 0.00057590, Val loss: 0.00031166, Val BER: 0.000097331, Time: 67.3s, Current: 01:13, ETA: 04:47\n",
      "Epoch: 61, Train loss: 0.00059570, Val loss: 0.00027338, Val BER: 0.000088411, Time: 67.4s, Current: 01:14, ETA: 04:47\n",
      "Epoch: 62, Train loss: 0.00057470, Val loss: 0.00022141, Val BER: 0.000071061, Time: 67.2s, Current: 01:15, ETA: 04:47\n",
      "Epoch: 63, Train loss: 0.00055936, Val loss: 0.00023620, Val BER: 0.000077539, Time: 67.4s, Current: 01:16, ETA: 04:47\n",
      "Epoch: 64, Train loss: 0.00054245, Val loss: 0.00024748, Val BER: 0.000075358, Time: 67.7s, Current: 01:17, ETA: 04:47\n",
      "Epoch: 65, Train loss: 0.00052722, Val loss: 0.00024053, Val BER: 0.000076562, Time: 67.3s, Current: 01:18, ETA: 04:47\n",
      "Epoch: 66, Train loss: 0.00051854, Val loss: 0.00025731, Val BER: 0.000080632, Time: 67.4s, Current: 01:19, ETA: 04:47\n",
      "Epoch: 67, Train loss: 0.00050201, Val loss: 0.00024558, Val BER: 0.000078092, Time: 67.3s, Current: 01:20, ETA: 04:47\n",
      "Epoch: 68, Train loss: 0.00052840, Val loss: 0.00025491, Val BER: 0.000081348, Time: 67.4s, Current: 01:22, ETA: 04:47\n",
      "Epoch: 69, Train loss: 0.00051827, Val loss: 0.00025000, Val BER: 0.000079948, Time: 67.4s, Current: 01:23, ETA: 04:47\n",
      "\n",
      "Learning Rate changed from 1.00e-03 to 5.00e-04 at Epoch 69\n",
      "Epoch: 70, Train loss: 0.00026642, Val loss: 0.00009589, Val BER: 0.000029329, Time: 67.2s, Current: 01:24, ETA: 04:47\n",
      "Epoch: 71, Train loss: 0.00019685, Val loss: 0.00009618, Val BER: 0.000030111, Time: 67.3s, Current: 01:25, ETA: 04:47\n",
      "Epoch: 72, Train loss: 0.00020054, Val loss: 0.00009487, Val BER: 0.000029362, Time: 67.3s, Current: 01:26, ETA: 04:47\n",
      "Epoch: 73, Train loss: 0.00021374, Val loss: 0.00008351, Val BER: 0.000026270, Time: 67.3s, Current: 01:27, ETA: 04:47\n",
      "Epoch: 74, Train loss: 0.00022906, Val loss: 0.00010261, Val BER: 0.000032031, Time: 67.3s, Current: 01:28, ETA: 04:47\n",
      "Epoch: 75, Train loss: 0.00023103, Val loss: 0.00009661, Val BER: 0.000028874, Time: 67.2s, Current: 01:29, ETA: 04:47\n",
      "Epoch: 76, Train loss: 0.00022701, Val loss: 0.00011337, Val BER: 0.000033236, Time: 67.2s, Current: 01:31, ETA: 04:47\n",
      "Epoch: 77, Train loss: 0.00023179, Val loss: 0.00010015, Val BER: 0.000030371, Time: 67.6s, Current: 01:32, ETA: 04:47\n",
      "Epoch: 78, Train loss: 0.00023790, Val loss: 0.00009949, Val BER: 0.000030892, Time: 67.3s, Current: 01:33, ETA: 04:47\n",
      "Epoch: 79, Train loss: 0.00023646, Val loss: 0.00010949, Val BER: 0.000034928, Time: 67.3s, Current: 01:34, ETA: 04:47\n",
      "Epoch: 80, Train loss: 0.00023513, Val loss: 0.00010727, Val BER: 0.000033854, Time: 67.1s, Current: 01:35, ETA: 04:47\n",
      "\n",
      "Learning Rate changed from 5.00e-04 to 2.50e-04 at Epoch 80\n",
      "Epoch: 81, Train loss: 0.00015321, Val loss: 0.00006001, Val BER: 0.000020052, Time: 67.2s, Current: 01:36, ETA: 04:46\n",
      "Epoch: 82, Train loss: 0.00014192, Val loss: 0.00004920, Val BER: 0.000014746, Time: 67.0s, Current: 01:37, ETA: 04:46\n",
      "Epoch: 83, Train loss: 0.00012717, Val loss: 0.00005376, Val BER: 0.000016113, Time: 67.1s, Current: 01:38, ETA: 04:46\n",
      "Epoch: 84, Train loss: 0.00012634, Val loss: 0.00004562, Val BER: 0.000014941, Time: 67.4s, Current: 01:40, ETA: 04:46\n",
      "Epoch: 85, Train loss: 0.00012046, Val loss: 0.00005563, Val BER: 0.000017318, Time: 67.3s, Current: 01:41, ETA: 04:46\n",
      "Epoch: 86, Train loss: 0.00012568, Val loss: 0.00004335, Val BER: 0.000013672, Time: 67.3s, Current: 01:42, ETA: 04:46\n",
      "Epoch: 87, Train loss: 0.00012787, Val loss: 0.00005310, Val BER: 0.000016016, Time: 67.3s, Current: 01:43, ETA: 04:46\n",
      "Epoch: 88, Train loss: 0.00012455, Val loss: 0.00005899, Val BER: 0.000018587, Time: 67.2s, Current: 01:44, ETA: 04:46\n",
      "Epoch: 89, Train loss: 0.00013454, Val loss: 0.00004730, Val BER: 0.000014648, Time: 67.3s, Current: 01:45, ETA: 04:46\n",
      "Epoch: 90, Train loss: 0.00013745, Val loss: 0.00004914, Val BER: 0.000015104, Time: 67.8s, Current: 01:46, ETA: 04:46\n",
      "Epoch: 91, Train loss: 0.00012646, Val loss: 0.00004853, Val BER: 0.000014909, Time: 67.3s, Current: 01:47, ETA: 04:46\n",
      "Epoch: 94, Train loss: 0.00010672, Val loss: 0.00003375, Val BER: 0.000010840, Time: 67.2s, Current: 01:51, ETA: 04:46\n",
      "Epoch: 95, Train loss: 0.00009690, Val loss: 0.00004136, Val BER: 0.000012174, Time: 67.4s, Current: 01:52, ETA: 04:46\n",
      "Epoch: 96, Train loss: 0.00009379, Val loss: 0.00003807, Val BER: 0.000011068, Time: 67.2s, Current: 01:53, ETA: 04:46\n",
      "Epoch: 97, Train loss: 0.00008868, Val loss: 0.00003452, Val BER: 0.000010156, Time: 67.3s, Current: 01:54, ETA: 04:46\n",
      "Epoch: 98, Train loss: 0.00008592, Val loss: 0.00003293, Val BER: 0.000010417, Time: 67.8s, Current: 01:55, ETA: 04:46\n",
      "Epoch: 99, Train loss: 0.00007920, Val loss: 0.00003189, Val BER: 0.000010254, Time: 67.5s, Current: 01:56, ETA: 04:46\n",
      "Epoch: 100, Train loss: 0.00007455, Val loss: 0.00003274, Val BER: 0.000010319, Time: 67.3s, Current: 01:58, ETA: 04:46\n",
      "Epoch: 101, Train loss: 0.00008034, Val loss: 0.00003422, Val BER: 0.000011165, Time: 67.3s, Current: 01:59, ETA: 04:46\n",
      "Epoch: 102, Train loss: 0.00008978, Val loss: 0.00002971, Val BER: 0.000009017, Time: 67.5s, Current: 02:00, ETA: 04:46\n",
      "BER首次达到阈值 1e-05 (当前值: 9.02e-06)\n",
      "通知邮件已发送\n",
      "Epoch: 103, Train loss: 0.00007953, Val loss: 0.00003795, Val BER: 0.000010449, Time: 67.2s, Current: 02:01, ETA: 04:46\n",
      "Epoch: 104, Train loss: 0.00008607, Val loss: 0.00003405, Val BER: 0.000010645, Time: 67.4s, Current: 02:02, ETA: 04:46\n",
      "Epoch: 105, Train loss: 0.00008247, Val loss: 0.00003388, Val BER: 0.000010286, Time: 67.5s, Current: 02:03, ETA: 04:46\n",
      "Epoch: 106, Train loss: 0.00008453, Val loss: 0.00003086, Val BER: 0.000009766, Time: 67.4s, Current: 02:04, ETA: 04:46\n",
      "Epoch: 107, Train loss: 0.00008262, Val loss: 0.00003216, Val BER: 0.000010156, Time: 67.3s, Current: 02:05, ETA: 04:46\n",
      "Epoch: 108, Train loss: 0.00008421, Val loss: 0.00003044, Val BER: 0.000009473, Time: 67.3s, Current: 02:07, ETA: 04:46\n",
      "Epoch: 109, Train loss: 0.00007501, Val loss: 0.00003118, Val BER: 0.000009635, Time: 67.5s, Current: 02:08, ETA: 04:46\n",
      "\n",
      "Learning Rate changed from 1.25e-04 to 6.25e-05 at Epoch 109\n",
      "Epoch: 110, Train loss: 0.00007431, Val loss: 0.00002961, Val BER: 0.000008724, Time: 67.1s, Current: 02:09, ETA: 04:46\n",
      "Epoch: 111, Train loss: 0.00006739, Val loss: 0.00002430, Val BER: 0.000007324, Time: 67.2s, Current: 02:10, ETA: 04:46\n",
      "Epoch: 112, Train loss: 0.00006263, Val loss: 0.00002253, Val BER: 0.000007129, Time: 67.2s, Current: 02:11, ETA: 04:46\n",
      "Epoch: 113, Train loss: 0.00006057, Val loss: 0.00002524, Val BER: 0.000007617, Time: 67.1s, Current: 02:12, ETA: 04:46\n",
      "Epoch: 114, Train loss: 0.00006269, Val loss: 0.00002348, Val BER: 0.000007650, Time: 67.7s, Current: 02:13, ETA: 04:46\n",
      "Epoch: 115, Train loss: 0.00006057, Val loss: 0.00002128, Val BER: 0.000006445, Time: 67.3s, Current: 02:14, ETA: 04:46\n",
      "Epoch: 116, Train loss: 0.00006274, Val loss: 0.00002402, Val BER: 0.000007259, Time: 67.2s, Current: 02:16, ETA: 04:46\n",
      "Epoch: 117, Train loss: 0.00006112, Val loss: 0.00002226, Val BER: 0.000006868, Time: 67.3s, Current: 02:17, ETA: 04:46\n",
      "Epoch: 118, Train loss: 0.00006264, Val loss: 0.00002455, Val BER: 0.000007715, Time: 67.4s, Current: 02:18, ETA: 04:46\n",
      "Epoch: 119, Train loss: 0.00005722, Val loss: 0.00002594, Val BER: 0.000007845, Time: 67.2s, Current: 02:19, ETA: 04:46\n",
      "Epoch: 120, Train loss: 0.00005655, Val loss: 0.00002355, Val BER: 0.000007357, Time: 67.4s, Current: 02:20, ETA: 04:46\n",
      "Epoch: 121, Train loss: 0.00005701, Val loss: 0.00002512, Val BER: 0.000008073, Time: 67.2s, Current: 02:21, ETA: 04:46\n",
      "Epoch: 122, Train loss: 0.00006815, Val loss: 0.00002469, Val BER: 0.000007585, Time: 67.6s, Current: 02:22, ETA: 04:46\n",
      "\n",
      "Learning Rate changed from 6.25e-05 to 3.13e-05 at Epoch 122\n",
      "Epoch: 123, Train loss: 0.00005618, Val loss: 0.00002274, Val BER: 0.000007064, Time: 67.3s, Current: 02:23, ETA: 04:46\n",
      "Epoch: 124, Train loss: 0.00005866, Val loss: 0.00002166, Val BER: 0.000006966, Time: 67.4s, Current: 02:25, ETA: 04:46\n",
      "Epoch: 125, Train loss: 0.00005569, Val loss: 0.00002681, Val BER: 0.000007454, Time: 67.3s, Current: 02:26, ETA: 04:46\n",
      "Epoch: 126, Train loss: 0.00005420, Val loss: 0.00002406, Val BER: 0.000007357, Time: 67.1s, Current: 02:27, ETA: 04:46\n",
      "Epoch: 127, Train loss: 0.00005100, Val loss: 0.00002453, Val BER: 0.000007585, Time: 67.2s, Current: 02:28, ETA: 04:46\n",
      "Epoch: 128, Train loss: 0.00005293, Val loss: 0.00002250, Val BER: 0.000006999, Time: 67.1s, Current: 02:29, ETA: 04:46\n",
      "Epoch: 129, Train loss: 0.00006129, Val loss: 0.00002306, Val BER: 0.000006836, Time: 67.1s, Current: 02:30, ETA: 04:46\n",
      "\n",
      "Learning Rate changed from 3.13e-05 to 1.56e-05 at Epoch 129\n",
      "Epoch: 130, Train loss: 0.00004890, Val loss: 0.00002350, Val BER: 0.000007031, Time: 67.6s, Current: 02:31, ETA: 04:46\n",
      "Epoch: 131, Train loss: 0.00005885, Val loss: 0.00002270, Val BER: 0.000006868, Time: 67.4s, Current: 02:32, ETA: 04:46\n",
      "Epoch: 132, Train loss: 0.00005144, Val loss: 0.00001871, Val BER: 0.000005632, Time: 67.5s, Current: 02:34, ETA: 04:46\n",
      "Epoch: 133, Train loss: 0.00005289, Val loss: 0.00002093, Val BER: 0.000006836, Time: 67.3s, Current: 02:35, ETA: 04:46\n",
      "Epoch: 134, Train loss: 0.00004832, Val loss: 0.00002135, Val BER: 0.000006510, Time: 67.4s, Current: 02:36, ETA: 04:46\n",
      "Epoch: 135, Train loss: 0.00004566, Val loss: 0.00002030, Val BER: 0.000006348, Time: 67.4s, Current: 02:37, ETA: 04:46\n",
      "Epoch: 136, Train loss: 0.00005044, Val loss: 0.00001754, Val BER: 0.000005436, Time: 67.5s, Current: 02:38, ETA: 04:46\n",
      "Epoch: 137, Train loss: 0.00004662, Val loss: 0.00002434, Val BER: 0.000006966, Time: 67.2s, Current: 02:39, ETA: 04:46\n",
      "Epoch: 138, Train loss: 0.00005194, Val loss: 0.00001718, Val BER: 0.000005534, Time: 67.4s, Current: 02:40, ETA: 04:46\n",
      "Epoch: 139, Train loss: 0.00004988, Val loss: 0.00001749, Val BER: 0.000005599, Time: 67.3s, Current: 02:41, ETA: 04:46\n",
      "Epoch: 140, Train loss: 0.00005333, Val loss: 0.00002009, Val BER: 0.000006543, Time: 67.3s, Current: 02:43, ETA: 04:46\n",
      "Epoch: 141, Train loss: 0.00005022, Val loss: 0.00001938, Val BER: 0.000005762, Time: 67.4s, Current: 02:44, ETA: 04:46\n",
      "Epoch: 142, Train loss: 0.00005300, Val loss: 0.00002371, Val BER: 0.000006543, Time: 67.4s, Current: 02:45, ETA: 04:46\n",
      "Epoch: 143, Train loss: 0.00004712, Val loss: 0.00001774, Val BER: 0.000005729, Time: 67.4s, Current: 02:46, ETA: 04:46\n",
      "Epoch: 144, Train loss: 0.00004725, Val loss: 0.00001997, Val BER: 0.000006120, Time: 67.3s, Current: 02:47, ETA: 04:46\n",
      "Epoch: 145, Train loss: 0.00004864, Val loss: 0.00002253, Val BER: 0.000006185, Time: 67.5s, Current: 02:48, ETA: 04:46\n",
      "\n",
      "Learning Rate changed from 1.56e-05 to 7.81e-06 at Epoch 145\n",
      "Epoch: 146, Train loss: 0.00004679, Val loss: 0.00001734, Val BER: 0.000005762, Time: 67.5s, Current: 02:49, ETA: 04:46\n",
      "Epoch: 147, Train loss: 0.00005809, Val loss: 0.00001657, Val BER: 0.000005111, Time: 67.2s, Current: 02:50, ETA: 04:46\n",
      "Epoch: 148, Train loss: 0.00004787, Val loss: 0.00002075, Val BER: 0.000006738, Time: 67.3s, Current: 02:52, ETA: 04:46\n",
      "Epoch: 149, Train loss: 0.00004970, Val loss: 0.00001992, Val BER: 0.000006380, Time: 67.3s, Current: 02:53, ETA: 04:46\n",
      "Epoch: 150, Train loss: 0.00004401, Val loss: 0.00001729, Val BER: 0.000005241, Time: 67.3s, Current: 02:54, ETA: 04:46\n",
      "Epoch: 151, Train loss: 0.00004812, Val loss: 0.00001710, Val BER: 0.000005859, Time: 67.3s, Current: 02:55, ETA: 04:46\n",
      "Epoch: 152, Train loss: 0.00004676, Val loss: 0.00001811, Val BER: 0.000005924, Time: 67.5s, Current: 02:56, ETA: 04:46\n",
      "Epoch: 153, Train loss: 0.00004699, Val loss: 0.00001786, Val BER: 0.000005111, Time: 67.2s, Current: 02:57, ETA: 04:46\n",
      "Epoch: 154, Train loss: 0.00004246, Val loss: 0.00001896, Val BER: 0.000005729, Time: 67.2s, Current: 02:58, ETA: 04:46\n",
      "\n",
      "Learning Rate changed from 7.81e-06 to 3.91e-06 at Epoch 154\n",
      "Epoch: 155, Train loss: 0.00004842, Val loss: 0.00001644, Val BER: 0.000005143, Time: 67.0s, Current: 02:59, ETA: 04:46\n",
      "Epoch: 156, Train loss: 0.00004806, Val loss: 0.00002033, Val BER: 0.000005306, Time: 67.2s, Current: 03:01, ETA: 04:46\n",
      "Epoch: 157, Train loss: 0.00004854, Val loss: 0.00001826, Val BER: 0.000005664, Time: 67.1s, Current: 03:02, ETA: 04:46\n",
      "Epoch: 158, Train loss: 0.00004335, Val loss: 0.00002149, Val BER: 0.000006608, Time: 67.2s, Current: 03:03, ETA: 04:46\n",
      "Epoch: 159, Train loss: 0.00004167, Val loss: 0.00001711, Val BER: 0.000004915, Time: 67.1s, Current: 03:04, ETA: 04:46\n",
      "Epoch: 160, Train loss: 0.00004721, Val loss: 0.00001691, Val BER: 0.000005208, Time: 66.9s, Current: 03:05, ETA: 04:46\n",
      "Epoch: 161, Train loss: 0.00004947, Val loss: 0.00001913, Val BER: 0.000005729, Time: 67.1s, Current: 03:06, ETA: 04:46\n",
      "Epoch: 162, Train loss: 0.00004383, Val loss: 0.00001960, Val BER: 0.000005534, Time: 67.1s, Current: 03:07, ETA: 04:46\n",
      "\n",
      "Learning Rate changed from 3.91e-06 to 1.95e-06 at Epoch 162\n",
      "Epoch: 163, Train loss: 0.00004956, Val loss: 0.00001998, Val BER: 0.000006445, Time: 67.0s, Current: 03:08, ETA: 04:45\n",
      "Epoch: 165, Train loss: 0.00004639, Val loss: 0.00001989, Val BER: 0.000005664, Time: 67.1s, Current: 03:11, ETA: 04:45\n",
      "Epoch: 166, Train loss: 0.00004263, Val loss: 0.00001573, Val BER: 0.000004753, Time: 67.1s, Current: 03:12, ETA: 04:45\n",
      "Epoch: 167, Train loss: 0.00004565, Val loss: 0.00001922, Val BER: 0.000005306, Time: 67.1s, Current: 03:13, ETA: 04:45\n",
      "Epoch: 168, Train loss: 0.00004808, Val loss: 0.00001797, Val BER: 0.000005697, Time: 66.9s, Current: 03:14, ETA: 04:45\n",
      "Epoch: 169, Train loss: 0.00004472, Val loss: 0.00001848, Val BER: 0.000005469, Time: 66.9s, Current: 03:15, ETA: 04:45\n",
      "Epoch: 170, Train loss: 0.00004352, Val loss: 0.00001965, Val BER: 0.000006022, Time: 67.3s, Current: 03:16, ETA: 04:45\n",
      "Epoch: 171, Train loss: 0.00004076, Val loss: 0.00002266, Val BER: 0.000006868, Time: 67.1s, Current: 03:17, ETA: 04:45\n",
      "Epoch: 172, Train loss: 0.00004587, Val loss: 0.00001895, Val BER: 0.000006445, Time: 67.0s, Current: 03:18, ETA: 04:45\n",
      "Epoch: 173, Train loss: 0.00004377, Val loss: 0.00002106, Val BER: 0.000006217, Time: 67.0s, Current: 03:20, ETA: 04:45\n",
      "\n",
      "Learning Rate changed from 1.95e-06 to 1.00e-06 at Epoch 173\n",
      "Epoch: 174, Train loss: 0.00004244, Val loss: 0.00001848, Val BER: 0.000005501, Time: 67.1s, Current: 03:21, ETA: 04:45\n",
      "Epoch: 175, Train loss: 0.00004597, Val loss: 0.00001760, Val BER: 0.000005697, Time: 66.9s, Current: 03:22, ETA: 04:45\n",
      "Epoch: 176, Train loss: 0.00004818, Val loss: 0.00001612, Val BER: 0.000004818, Time: 67.0s, Current: 03:23, ETA: 04:45\n",
      "Epoch: 177, Train loss: 0.00004380, Val loss: 0.00002076, Val BER: 0.000005013, Time: 67.1s, Current: 03:24, ETA: 04:45\n",
      "Epoch: 178, Train loss: 0.00004752, Val loss: 0.00001814, Val BER: 0.000005501, Time: 67.0s, Current: 03:25, ETA: 04:45\n",
      "Epoch: 179, Train loss: 0.00004451, Val loss: 0.00001731, Val BER: 0.000005176, Time: 67.2s, Current: 03:26, ETA: 04:45\n",
      "Epoch: 180, Train loss: 0.00003985, Val loss: 0.00001882, Val BER: 0.000005827, Time: 67.6s, Current: 03:27, ETA: 04:45\n",
      "Epoch: 181, Train loss: 0.00004650, Val loss: 0.00001526, Val BER: 0.000005143, Time: 67.3s, Current: 03:29, ETA: 04:45\n",
      "Epoch: 182, Train loss: 0.00004507, Val loss: 0.00001858, Val BER: 0.000005762, Time: 67.3s, Current: 03:30, ETA: 04:45\n",
      "Epoch: 183, Train loss: 0.00004588, Val loss: 0.00002206, Val BER: 0.000006510, Time: 67.2s, Current: 03:31, ETA: 04:45\n",
      "Epoch: 184, Train loss: 0.00004420, Val loss: 0.00002001, Val BER: 0.000006087, Time: 67.1s, Current: 03:32, ETA: 04:45\n",
      "Epoch: 185, Train loss: 0.00004216, Val loss: 0.00001821, Val BER: 0.000005794, Time: 67.2s, Current: 03:33, ETA: 04:45\n",
      "Epoch: 186, Train loss: 0.00004560, Val loss: 0.00001626, Val BER: 0.000005046, Time: 67.3s, Current: 03:34, ETA: 04:45\n",
      "Epoch: 187, Train loss: 0.00004534, Val loss: 0.00001629, Val BER: 0.000005404, Time: 67.3s, Current: 03:35, ETA: 04:45\n",
      "Epoch: 188, Train loss: 0.00004216, Val loss: 0.00001547, Val BER: 0.000005013, Time: 67.3s, Current: 03:36, ETA: 04:45\n",
      "Epoch: 189, Train loss: 0.00004494, Val loss: 0.00001593, Val BER: 0.000005241, Time: 67.3s, Current: 03:38, ETA: 04:45\n",
      "Epoch: 190, Train loss: 0.00004696, Val loss: 0.00001869, Val BER: 0.000004948, Time: 67.2s, Current: 03:39, ETA: 04:45\n",
      "Early stopping triggered!\n",
      "Training completed at 2025-10-04 03:39:11\n",
      "Training finished\n",
      "\n",
      "==================================================\n",
      "Testing Best BER Model\n",
      "测试开始时间(北京时间): 2025-10-04 03:39:12\n",
      "\n",
      "[进度更新 03:39:12]\n",
      "当前进度: 0.0% | 已完成 1/1 SNR点\n",
      "当前SNR: 20dB | 当前批次: 1/50000\n",
      "预计剩余时间: 34.4分钟 | 总预计耗时: 34.4分钟\n",
      "最新预计结束时间(北京时间): 2025-10-04 04:13:33\n",
      "\n",
      "[进度更新 03:39:20]\n",
      "当前进度: 0.4% | 已完成 1/1 SNR点\n",
      "当前SNR: 20dB | 当前批次: 200/50000\n",
      "预计剩余时间: 33.2分钟 | 总预计耗时: 33.3分钟\n",
      "最新预计结束时间(北京时间): 2025-10-04 04:12:29\n",
      "\n",
      "[进度更新 03:39:28]\n",
      "当前进度: 0.8% | 已完成 1/1 SNR点\n",
      "当前SNR: 20dB | 当前批次: 400/50000\n",
      "预计剩余时间: 33.1分钟 | 总预计耗时: 33.3分钟\n",
      "最新预计结束时间(北京时间): 2025-10-04 04:12:31\n",
      "\n",
      "[进度更新 03:39:36]\n",
      "当前进度: 1.2% | 已完成 1/1 SNR点\n",
      "当前SNR: 20dB | 当前批次: 600/50000\n",
      "预计剩余时间: 33.1分钟 | 总预计耗时: 33.5分钟\n",
      "最新预计结束时间(北京时间): 2025-10-04 04:12:39\n",
      "\n",
      "[进度更新 03:39:44]\n",
      "当前进度: 1.6% | 已完成 1/1 SNR点\n",
      "当前SNR: 20dB | 当前批次: 800/50000\n",
      "预计剩余时间: 32.9分钟 | 总预计耗时: 33.4分钟\n",
      "最新预计结束时间(北京时间): 2025-10-04 04:12:36\n",
      "\n",
      "[进度更新 04:12:36]\n",
      "当前进度: 100.0% | 已完成 1/1 SNR点\n",
      "当前SNR: 20dB | 当前批次: 50000/50000\n",
      "预计剩余时间: 0.0分钟 | 总预计耗时: 33.4分钟\n",
      "最新预计结束时间(北京时间): 2025-10-04 04:12:36\n",
      "\n",
      "SNR点  20 dB 测试完成:\n",
      "BER: 8.6699e-07 | 当前SNR点耗时: 33.4分钟\n",
      "平均每批次时间: 0.040s\n",
      "\n",
      "测试实际结束时间(北京时间): 2025-10-04 04:12:36\n",
      "总测试耗时: 0.56小时 (33.4分钟)\n",
      "平均每个SNR点耗时: 33.4分钟\n",
      "测试结果邮件已发送\n",
      "BER results: [8.669921875e-07]\n",
      "\n",
      "==================================================\n",
      "Model Summary (Best BER Model)\n",
      "========================================================================================================================\n",
      "Layer (type:depth-idx)                                                 Output Shape              Param #\n",
      "========================================================================================================================\n",
      "Net                                                                    [1, 1, 128]               --\n",
      "├─ComplexEncoder: 1-1                                                  [1, 2, 128]               --\n",
      "│    └─RealToComplex: 2-1                                              [1, 1, 128]               --\n",
      "│    │    └─Linear: 3-1                                                [1, 128]                  16,512\n",
      "│    │    └─Linear: 3-2                                                [1, 128]                  16,512\n",
      "│    └─Sequential: 2-2                                                 [1, 32, 128]              --\n",
      "│    │    └─ComplexConv1d: 3-3                                         [1, 256, 128]             3,072\n",
      "│    │    └─ComplexReLU: 3-4                                           [1, 256, 128]             --\n",
      "│    │    └─ComplexConv1d: 3-5                                         [1, 64, 128]              24,704\n",
      "│    │    └─ComplexReLU: 3-6                                           [1, 64, 128]              --\n",
      "│    │    └─ComplexConv1d: 3-7                                         [1, 32, 128]              3,136\n",
      "│    └─ComplexConv1d: 2-3                                              [1, 32, 128]              --\n",
      "│    │    └─Conv1d: 3-8                                                [1, 32, 128]              128\n",
      "│    │    └─Conv1d: 3-9                                                [1, 32, 128]              128\n",
      "│    │    └─Conv1d: 3-10                                               [1, 32, 128]              (recursive)\n",
      "│    │    └─Conv1d: 3-11                                               [1, 32, 128]              (recursive)\n",
      "│    └─Sequential: 2-4                                                 [1, 128, 2]               --\n",
      "│    │    └─ReLU: 3-12                                                 [1, 128, 64]              --\n",
      "│    │    └─TimeDistributed: 3-13                                      [1, 128, 2]               130\n",
      "│    │    └─BatchNorm1d: 3-14                                          [1, 128, 2]               256\n",
      "├─PreAttn: 1-2                                                         [1, 70, 135]              --\n",
      "│    └─ComplexMultiScaleConvBlock: 2-5                                 [1, 105, 135]             --\n",
      "│    │    └─ComplexConv1d: 3-15                                        [1, 35, 135]              280\n",
      "│    │    └─NaiveComplexBatchNorm1d: 3-16                              [1, 35, 135]              140\n",
      "│    │    └─ComplexReLU: 3-17                                          [1, 35, 135]              --\n",
      "│    │    └─ComplexConv1d: 3-18                                        [1, 35, 135]              420\n",
      "│    │    └─NaiveComplexBatchNorm1d: 3-19                              [1, 35, 135]              140\n",
      "│    │    └─ComplexReLU: 3-20                                          [1, 35, 135]              --\n",
      "│    │    └─ComplexConv1d: 3-21                                        [1, 35, 135]              560\n",
      "│    │    └─NaiveComplexBatchNorm1d: 3-22                              [1, 35, 135]              140\n",
      "│    │    └─ComplexReLU: 3-23                                          [1, 35, 135]              --\n",
      "│    └─ComplexConv1d: 2-6                                              [1, 35, 135]              --\n",
      "│    │    └─Conv1d: 3-24                                               [1, 35, 135]              3,710\n",
      "│    │    └─Conv1d: 3-25                                               [1, 35, 135]              3,710\n",
      "│    │    └─Conv1d: 3-26                                               [1, 35, 135]              (recursive)\n",
      "│    │    └─Conv1d: 3-27                                               [1, 35, 135]              (recursive)\n",
      "│    └─NaiveComplexBatchNorm1d: 2-7                                    [1, 35, 135]              --\n",
      "│    │    └─BatchNorm1d: 3-28                                          [1, 35, 135]              70\n",
      "│    │    └─BatchNorm1d: 3-29                                          [1, 35, 135]              70\n",
      "│    └─ComplexReLU: 2-8                                                [1, 35, 135]              --\n",
      "│    └─SE: 2-9                                                         [1, 35, 135]              --\n",
      "│    │    └─AdaptiveAvgPool1d: 3-30                                    [1, 70, 1]                --\n",
      "│    │    └─Conv1d: 3-31                                               [1, 35, 1]                2,485\n",
      "│    │    └─ReLU: 3-32                                                 [1, 35, 1]                --\n",
      "│    │    └─Conv1d: 3-33                                               [1, 70, 1]                2,520\n",
      "│    │    └─Sigmoid: 3-34                                              [1, 70, 1]                --\n",
      "│    └─ComplexConv1d: 2-10                                             [1, 35, 135]              --\n",
      "│    │    └─Conv1d: 3-35                                               [1, 35, 135]              70\n",
      "│    │    └─Conv1d: 3-36                                               [1, 35, 135]              70\n",
      "│    │    └─Conv1d: 3-37                                               [1, 35, 135]              (recursive)\n",
      "│    │    └─Conv1d: 3-38                                               [1, 35, 135]              (recursive)\n",
      "├─Sequential: 1-3                                                      [1, 1, 128]               --\n",
      "│    └─BinaryVoVUnit: 2-11                                             [1, 35, 128]              --\n",
      "│    │    └─ModuleList: 3-39                                           --                        1,245\n",
      "│    │    └─Sequential: 3-40                                           [1, 35, 135]              3,080\n",
      "│    │    └─Sequential: 3-41                                           [1, 35, 128]              9,870\n",
      "│    │    └─AvgPool1d: 3-42                                            [1, 70, 128]              --\n",
      "│    │    └─Sequential: 3-43                                           [1, 35, 128]              2,520\n",
      "│    └─Decoder: 2-12                                                   [1, 1, 128]               --\n",
      "│    │    └─Transformer: 3-44                                          [1, 35, 128]              3,962,168\n",
      "│    │    └─Sequential: 3-45                                           [1, 128, 1]               36\n",
      "========================================================================================================================\n",
      "Total params: 4,057,882\n",
      "Trainable params: 4,057,882\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 68.45\n",
      "========================================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 6.13\n",
      "Params size (MB): 16.23\n",
      "Estimated Total Size (MB): 22.36\n",
      "========================================================================================================================\n",
      "thop-flops & trainable parameters: ['77.099M', '4.058M']\n"
     ]
    }
   ],
   "source": [
    "def H_sample(TDL_dB, delay, batch_size=None):\n",
    "    powerTDL = 10 ** (torch.tensor(TDL_dB).to(device) / 10)\n",
    "\n",
    "    if batch_size:\n",
    "        H_r = torch.zeros(batch_size, delay[-1] + 1, device=device)\n",
    "        H_i = torch.zeros(batch_size, delay[-1] + 1, device=device)\n",
    "        H_r[:, delay] = torch.randn(batch_size, len(delay), device=device) * torch.sqrt(powerTDL.unsqueeze(0) / 2)\n",
    "        H_i[:, delay] = torch.randn(batch_size, len(delay), device=device) * torch.sqrt(powerTDL.unsqueeze(0) / 2)\n",
    "\n",
    "    else:\n",
    "        H_r = torch.zeros(delay[-1] + 1, device=device) #[8]\n",
    "        H_i = torch.zeros(delay[-1] + 1, device=device)\n",
    "        H_r[delay] = torch.randn(len(delay)).to(device) * torch.sqrt(powerTDL / 2).to(device)\n",
    "        H_i[delay] = torch.randn(len(delay)).to(device) * torch.sqrt(powerTDL / 2).to(device)\n",
    "    return H_r, H_i #[batch_size, 8]\n",
    "\n",
    "def send_notification_email(filename_prefix, best_loss_value, epoch):\n",
    "    \"\"\"发送训练结果通知邮件\"\"\"\n",
    "    try:\n",
    "        # 邮件配置\n",
    "        sender = '@163.com'\n",
    "        receiver = '8@qq.com'  # 可以改为你的接收邮箱\n",
    "        password = 'BV'  # 授权码\n",
    "        \n",
    "        # 邮件内容\n",
    "        subject = f\"训练达标通知 - {filename_prefix}\"\n",
    "        body = (f\"训练已达到目标值!\\n\\n\"\n",
    "               f\"训练标识: {filename_prefix}\\n\"\n",
    "               f\"当前epoch: {epoch}\\n\"\n",
    "               f\"best_loss值: {best_loss_value:.2e}\\n\"\n",
    "               f\"时间: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # 创建邮件\n",
    "        msg = MIMEText(body, 'plain', 'utf-8')\n",
    "        msg['From'] = sender\n",
    "        msg['To'] = receiver\n",
    "        msg['Subject'] = subject\n",
    "        \n",
    "        # 发送邮件\n",
    "        with smtplib.SMTP_SSL('smtp.163.com', 465) as server:\n",
    "            server.login(sender, password)\n",
    "            server.sendmail(sender, receiver, msg.as_string())\n",
    "        \n",
    "        print(\"通知邮件已发送\")\n",
    "    except Exception as e:\n",
    "        print(f\"发送邮件失败: {e}\")\n",
    "\n",
    "def train(train_SNR, batch_size, batch_num, lr, epochs, TDL_dB, delay, sample_num):\n",
    "    # 初始化参数（保持不变）\n",
    "    train_SNR = train_SNR\n",
    "    batch_size = batch_size\n",
    "    batch_num = batch_num\n",
    "    lr = lr\n",
    "    epochs = epochs\n",
    "    TDL_dB = TDL_dB\n",
    "    delay = delay\n",
    "    sample_num = sample_num\n",
    "    ber_threshold = 1e-5\n",
    "    threshold_reached = False\n",
    "    # 生成文件名前缀\n",
    "    filename_prefix = generate_filename_prefix(batch_size, batch_num, epochs, lr, sample_num, seed)\n",
    "\n",
    "    # 创建日志文件\n",
    "    log_filename = f'{filename_prefix}training_log.txt'\n",
    "    with open(log_filename, 'w') as log_file:\n",
    "        log_file.write(f\"Training Log - {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log_file.write(f\"Parameters: train_SNR={train_SNR}, batch_size={batch_size}, batch_num={batch_num}, \"\n",
    "                      f\"lr={lr}, epochs={epochs}, TDL_dB={TDL_dB}, delay={delay}, sample_num={sample_num}\\n\\n\")\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "    if os.path.exists('best.pth'):\n",
    "        net.load_state_dict(torch.load('best.pth', map_location=lambda storage, loc: storage.cuda(0)))\n",
    "        log_message = \"Loaded pre trained model 'best.pth'\"\n",
    "        print(log_message)\n",
    "        with open(log_filename, 'a') as log_file:\n",
    "            log_file.write(log_message + \"\\n\")\n",
    "    else:\n",
    "        log_message = \"No pre trained model found. Starting training from scratch.\"\n",
    "        print(log_message)\n",
    "        with open(log_filename, 'a') as log_file:\n",
    "            log_file.write(log_message + \"\\n\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=6, \n",
    "                                                          verbose=True, threshold=0.0001, threshold_mode='rel', \n",
    "                                                          cooldown=0, min_lr=1e-6, eps=1e-08)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    best_train_loss = torch.tensor(np.inf)\n",
    "    best_ber = 1.0  \n",
    "    ber_records = []\n",
    "\n",
    "    # 早停机制\n",
    "    epochs_no_improve = 0\n",
    "    stop_patience = 25\n",
    "\n",
    "    # 训练循环\n",
    "    start_train_time = time.time()\n",
    "    last_lr = lr\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 训练过程\n",
    "        for _ in range(batch_num):\n",
    "            train_data = torch.tensor(np.random.randint(0, 2, [batch_size, 128])).float().to(device)\n",
    "            s = train_data.reshape(batch_size, 1, -1)\n",
    "            optimizer.zero_grad()\n",
    "            BCE_loss = 0.0\n",
    "            for _ in range(sample_num):\n",
    "                H_r, H_i = H_sample(TDL_dB, delay, batch_size)\n",
    "                r = net(s, train_SNR, H_r, H_i, batch_size)\n",
    "                BCE_loss += criterion(r, s)\n",
    "            BCE_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += BCE_loss.item()/sample_num\n",
    "\n",
    "        train_loss /= batch_num\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # 验证过程\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        total_errors = 0\n",
    "        total_bits = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(batch_num):\n",
    "                val_data = torch.tensor(np.random.randint(0, 2, [batch_size, 128])).float().to(device)\n",
    "                s = val_data.reshape(batch_size, 1, -1)\n",
    "                H_r, H_i = H_sample(TDL_dB, delay, batch_size)\n",
    "                r = net(s, train_SNR, H_r, H_i, batch_size)\n",
    "                predicted = torch.round(r)\n",
    "                errors = torch.sum(torch.abs(predicted - s)).item()\n",
    "                val_loss += criterion(r, s).item()\n",
    "                total_errors += errors\n",
    "                total_bits += s.numel()\n",
    "        \n",
    "        val_loss /= batch_num\n",
    "        val_losses.append(val_loss)\n",
    "        ber = total_errors / total_bits\n",
    "        ber_records.append(ber)\n",
    "        \n",
    "        \n",
    "\n",
    "        # 计算时间信息        \n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "        elapsed_time = time.time() - start_train_time\n",
    "        average_time_per_epoch = elapsed_time / (epoch + 1)\n",
    "\n",
    "        # 固定按 250 轮计算剩余时间，但确保不会小于 0\n",
    "        remaining_epochs = max(250 - epoch - 1, 0)  # 防止负数\n",
    "        remaining_time = average_time_per_epoch * remaining_epochs\n",
    "\n",
    "        current_time = time.strftime(\"%H:%M\", time.localtime())\n",
    "        estimated_end_time = time.strftime(\"%H:%M\", time.localtime(time.time() + remaining_time))\n",
    "\n",
    "        # 准备日志信息\n",
    "        log_message = (f\"Epoch: {epoch}, \"\n",
    "                      f\"Train loss: {train_loss:.8f}, \"\n",
    "                      f\"Val loss: {val_loss:.8f}, \"\n",
    "                      f\"Val BER: {ber:.9f}, \"\n",
    "                      f\"Time: {epoch_duration:.1f}s, \"\n",
    "                      f\"Current: {current_time}, \"\n",
    "                      f\"ETA: {estimated_end_time}\")\n",
    "        \n",
    "        # 打印并记录日志\n",
    "        print(log_message)\n",
    "        with open(log_filename, 'a') as log_file:\n",
    "            log_file.write(log_message + \"\\n\")\n",
    "\n",
    "        # 保存最佳模型（三种指标）\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(net.state_dict(), f'{filename_prefix}_best_val_loss.pth')\n",
    "            with open(log_filename, 'a') as log_file:\n",
    "                log_file.write(f\"Best val loss model saved at epoch {epoch}\\n\")\n",
    "        \n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(net.state_dict(), f'{filename_prefix}_best_train_loss.pth')\n",
    "            with open(log_filename, 'a') as log_file:\n",
    "                log_file.write(f\"Best train loss model saved at epoch {epoch}\\n\")\n",
    "        \n",
    "        if ber < best_ber:\n",
    "            best_ber = ber\n",
    "            torch.save(net.state_dict(), f'{filename_prefix}_best.pth')\n",
    "            with open(log_filename, 'a') as log_file:\n",
    "                log_file.write(f\"Best BER model saved at epoch {epoch}\\n\")\n",
    "            \n",
    "            # 重置早停计数器（只有当BER有提升时才重置）\n",
    "            epochs_no_improve = 0\n",
    "        \n",
    "        # 新增 BER 阈值检查\n",
    "        if ber < ber_threshold and not threshold_reached:  # 假设 ber_threshold = 1e-5\n",
    "            threshold_reached = True\n",
    "            log_message = f\"BER首次达到阈值 {ber_threshold} (当前值: {ber:.2e})\"\n",
    "            print(log_message)\n",
    "            with open(log_filename, 'a') as log_file:\n",
    "                log_file.write(log_message + \"\\n\")\n",
    "            \n",
    "            # 发送通知邮件，传入filename_prefix\n",
    "            send_notification_email(filename_prefix, ber, epoch)\n",
    "\n",
    "        # 早停逻辑（保持不变）\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve == stop_patience:\n",
    "            stop_message = 'Early stopping triggered!'\n",
    "            print(stop_message)\n",
    "            with open(log_filename, 'a') as log_file:\n",
    "                log_file.write(stop_message + \"\\n\")\n",
    "            break\n",
    "\n",
    "        # 定期保存图表（保持不变）\n",
    "        if epoch % 5 == 4:\n",
    "            # 保存损失曲线图\n",
    "            plt.figure()\n",
    "            plt.plot(train_losses, label='Train Loss')\n",
    "            plt.plot(val_losses, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.yscale('log')\n",
    "            plt.title(f'Loss Curves (Epoch {epoch+1})')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'{filename_prefix}_loss_epoch.png')\n",
    "            plt.close()\n",
    "\n",
    "            # 保存BER曲线图\n",
    "            plt.figure()\n",
    "            plt.plot(ber_records, label='BER')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('BER')\n",
    "            plt.yscale('log')\n",
    "            plt.title(f'BER Curve (Epoch {epoch+1})')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'{filename_prefix}_ber_epoch.png')\n",
    "            plt.close()   \n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # 检查学习率是否发生变化\n",
    "        if epoch == 0:\n",
    "            last_lr = current_lr  # 初始化记录\n",
    "            print(f\"Initial Learning Rate: {current_lr:.2e}\")\n",
    "        elif current_lr != last_lr:\n",
    "            print(f\"\\nLearning Rate changed from {last_lr:.2e} to {current_lr:.2e} at Epoch {epoch}\")\n",
    "            last_lr = current_lr  # 更新记录\n",
    "\n",
    "    # 最终图表保存（保持不变）\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{filename_prefix}_loss_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ber_records, label='BER')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BER')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Bit Error Rate over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{filename_prefix}_val_BER.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 训练结束信息\n",
    "    end_message = f'Training completed at {time.strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "    print(end_message)\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(\"\\n\" + end_message + \"\\n\")\n",
    "        log_file.write(f\"Total training time: {time.time() - start_train_time:.2f} seconds\\n\")\n",
    "        log_file.write(f\"Best validation loss: {best_loss:.8f}\\n\")\n",
    "        log_file.write(f\"Final BER: {ber_records[-1]:.9f}\\n\")\n",
    "\n",
    "    print('Training finished')\n",
    "\n",
    "def send_test_results_email(filename_prefix, SNR_range, BER_results):\n",
    "    \"\"\"发送测试结果邮件\"\"\"\n",
    "    try:\n",
    "        # 邮件配置\n",
    "        sender = '@163.com'\n",
    "        receiver =  # 可以改为你的接收邮箱\n",
    "        password =  # 授权码\n",
    "        # 创建结果表格\n",
    "        result_table = \"SNR(dB)\\tBER\\n\"\n",
    "        result_table += \"-------\\t-------\\n\"\n",
    "        for snr, ber in zip(SNR_range, BER_results):\n",
    "            result_table += f\"{snr}\\t{ber:.4e}\\n\"\n",
    "        \n",
    "        # 邮件内容\n",
    "        subject = f\"模型测试结果 - {filename_prefix}\"\n",
    "        body = (f\"模型测试已完成!\\n\\n\"\n",
    "               f\"测试标识: {filename_prefix}\\n\"\n",
    "               f\"测试时间: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "               f\"测试结果:\\n{result_table}\\n\"\n",
    "               f\"最佳BER: {min(BER_results):.4e} (在SNR {SNR_range[BER_results.index(min(BER_results))]}dB时)\")\n",
    "        \n",
    "        # 创建邮件\n",
    "        msg = MIMEText(body, 'plain', 'utf-8')\n",
    "        msg['From'] = sender\n",
    "        msg['To'] = receiver\n",
    "        msg['Subject'] = subject\n",
    "        \n",
    "        # 发送邮件\n",
    "        with smtplib.SMTP_SSL('smtp.163.com', 465) as server:\n",
    "            server.login(sender, password)\n",
    "            server.sendmail(sender, receiver, msg.as_string())\n",
    "        \n",
    "        print(\"测试结果邮件已发送\")\n",
    "    except Exception as e:\n",
    "        print(f\"发送测试结果邮件失败: {e}\")\n",
    "\n",
    "\n",
    "def test_model(net, SNR_range, filename_prefix, num_samples=40000000, batch_size=800, log_filename=None):\n",
    "    \"\"\"\n",
    "    测试模型并保存BER结果到txt文件，同时发送邮件通知\n",
    "    \n",
    "    参数:\n",
    "        net: 训练好的模型\n",
    "        SNR_range: 要测试的SNR范围\n",
    "        num_samples: 总样本数\n",
    "        batch_size: 批处理大小\n",
    "        log_filename: 指定日志文件路径(优先使用)\n",
    "        filename_prefix: 如果log_filename为None，则用此前缀生成日志文件名\n",
    "    \"\"\"\n",
    "    BER = []\n",
    "    TDL_dB = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    delay = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    \n",
    "    # 记录测试开始时间\n",
    "    test_start_time = time.time()\n",
    "    print(f\"测试开始时间(北京时间): {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(test_start_time))}\")\n",
    "    \n",
    "    # 确定日志文件名\n",
    "    if log_filename is None:\n",
    "        if filename_prefix is not None:\n",
    "            log_filename = f'{filename_prefix}BER_results.txt'\n",
    "        else:\n",
    "            log_filename = 'BER_results.txt'\n",
    "            print(f\"警告: 未指定日志文件名，使用默认: {log_filename}\")\n",
    "    \n",
    "    # 测试过程\n",
    "    for idx, snr in enumerate(SNR_range):\n",
    "        snr_start_time = time.time()\n",
    "        total_errors = 0\n",
    "        total_bits = 0\n",
    "        \n",
    "        # 计算总批次数量\n",
    "        total_batches = (num_samples + batch_size - 1) // batch_size\n",
    "        \n",
    "        # 分批处理\n",
    "        for batch_idx in range(total_batches):\n",
    "            current_batch_size = min(batch_size, num_samples - batch_idx * batch_size)\n",
    "            \n",
    "            # 生成测试数据\n",
    "            data = torch.tensor(np.random.randint(0, 2, [current_batch_size, 128])).float().to(device)\n",
    "            s = data.reshape(current_batch_size, 1, -1)\n",
    "            \n",
    "            # 前向传播\n",
    "            with torch.no_grad():\n",
    "                H_r, H_i = H_sample(TDL_dB, delay, current_batch_size)\n",
    "                r = net(s, snr, H_r, H_i, current_batch_size)\n",
    "            \n",
    "            # 计算误码\n",
    "            predicted = torch.round(r)\n",
    "            errors = torch.sum(torch.abs(predicted - s)).item()\n",
    "            total_errors += errors\n",
    "            total_bits += s.numel()\n",
    "            \n",
    "            current_time = time.time()\n",
    "            \n",
    "            update_time_prediction = False\n",
    "\n",
    "            if idx == 0 and (batch_idx + 1 in {1, 200, 400, 600, 800} or batch_idx + 1 == total_batches):\n",
    "                update_time_prediction = True\n",
    "\n",
    "            if idx > 0 and (batch_idx + 1 in {1, 500, 1000, 1500, 2000} or batch_idx + 1 == total_batches):\n",
    "                update_time_prediction = True\n",
    "            \n",
    "            # 更新时间预测\n",
    "            if update_time_prediction:\n",
    "                elapsed = current_time - test_start_time\n",
    "                completed_samples = idx * num_samples + batch_idx * batch_size + current_batch_size\n",
    "                total_samples = len(SNR_range) * num_samples\n",
    "                completed = completed_samples / total_samples\n",
    "                \n",
    "                if completed > 0:\n",
    "                    total_estimated_time = elapsed / completed\n",
    "                    remaining_time = total_estimated_time - elapsed\n",
    "                    finish_time = current_time + remaining_time\n",
    "                    \n",
    "                    # 转换为北京时间\n",
    "                    finish_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(finish_time))\n",
    "                    current_time_str = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "                    \n",
    "                    # 打印进度信息\n",
    "                    print(f\"\\n[进度更新 {current_time_str}]\")\n",
    "                    print(f\"当前进度: {completed*100:.1f}% | 已完成 {idx+1}/{len(SNR_range)} SNR点\")\n",
    "                    print(f\"当前SNR: {snr}dB | 当前批次: {batch_idx+1}/{total_batches}\")\n",
    "                    print(f\"预计剩余时间: {remaining_time/60:.1f}分钟 | 总预计耗时: {total_estimated_time/60:.1f}分钟\")\n",
    "                    print(f\"最新预计结束时间(北京时间): {finish_time_str}\")\n",
    "        \n",
    "        # 计算BER\n",
    "        ber = total_errors / total_bits\n",
    "        BER.append(ber)\n",
    "        \n",
    "        # 打印当前SNR点完成信息\n",
    "        snr_time = time.time() - snr_start_time\n",
    "        print(f\"\\nSNR点 {snr:3d} dB 测试完成:\")\n",
    "        print(f\"BER: {ber:.4e} | 当前SNR点耗时: {snr_time/60:.1f}分钟\")\n",
    "        print(f\"平均每批次时间: {snr_time/total_batches:.3f}s\")\n",
    "    \n",
    "    # 测试实际结束时间\n",
    "    test_end_time = time.time()\n",
    "    total_test_time = test_end_time - test_start_time\n",
    "    print(f\"\\n测试实际结束时间(北京时间): {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(test_end_time))}\")\n",
    "    print(f\"总测试耗时: {total_test_time/3600:.2f}小时 ({total_test_time/60:.1f}分钟)\")\n",
    "    print(f\"平均每个SNR点耗时: {total_test_time/len(SNR_range)/60:.1f}分钟\")\n",
    "    \n",
    "    # 保存BER结果到txt文件\n",
    "    with open(log_filename, 'w') as f:\n",
    "        for snr, ber in zip(SNR_range, BER):\n",
    "            f.write(f\"{snr} {ber}\\n\")\n",
    "    \n",
    "    # 发送测试结果邮件\n",
    "    if filename_prefix is not None:\n",
    "        send_test_results_email(filename_prefix, SNR_range, BER)\n",
    "    else:\n",
    "        print(\"未指定filename_prefix，跳过发送邮件\")\n",
    "    \n",
    "    return BER\n",
    "\n",
    "def plot_ber(SNR_range, BER, batch_size, epochs, lr):\n",
    "    filename_prefix = generate_filename_prefix(batch_size, batch_num, epochs, lr, sample_num, seed)\n",
    "    plt.figure()\n",
    "    \n",
    "    # LS\n",
    "    LS_BER = [0.5, 0.35, 0.1, 0.0055, 3e-4]\n",
    "    # MMSE\n",
    "    MMSE_BER = [0.45, 0.3, 0.04, 1.5e-3, 5.5e-5]\n",
    "    # E2E\n",
    "    E2E_BER = [0.3, 0.15, 0.03, 1.5e-3, 1e-4]\n",
    "    # E2E-GAN\n",
    "    GAN_E2E_BER = [0.26, 0.12, 0.018, 6.6e-4, 3.3e-5]\n",
    "    # E2E-GT-noDense\n",
    "    GT_E2E_noDense_BER = [0.133296875, 0.042453125, 0.007496875, 0.000915625, 9.0625e-05]\n",
    "    # E2E-GT-noGhost\n",
    "    GT_E2E_noGhost_BER = [0.17920625, 0.06485, 0.010740625, 0.00100625, 0.000134375]\n",
    "    \n",
    "    # LS\n",
    "    plt.semilogy(SNR_range, LS_BER, marker='s', label='LS', linestyle='-', color='purple')\n",
    "    # MMSE\n",
    "    plt.semilogy(SNR_range, MMSE_BER, marker='o', label='MMSE', linestyle='-', color='blue')\n",
    "    # E2E\n",
    "    plt.semilogy(SNR_range, E2E_BER, marker='o', label='E2E', linestyle='-', color='magenta', markerfacecolor='none')\n",
    "    # E2E-GAN\n",
    "    plt.semilogy(SNR_range, GAN_E2E_BER, marker='s', label='E2E-GAN', linestyle='-', color='cyan', markerfacecolor='none')\n",
    "    # GT-E2E-noDense\n",
    "    plt.semilogy(SNR_range, GT_E2E_noDense_BER, marker='^', label='GT-E2E-no Dense', linestyle='--', color='red')\n",
    "    # GT-E2E-noGhost\n",
    "    plt.semilogy(SNR_range, GT_E2E_noGhost_BER, marker='v', label='GT-E2E-no Ghost', linestyle='--', color='red')\n",
    "\n",
    "    # GT-E2E\n",
    "    plt.semilogy(SNR_range, BER, marker='+', label='GT-moeE2E', linestyle='-', color='red')\n",
    "    \n",
    "    plt.xlabel('SNR (dB)')\n",
    "    plt.ylabel('BER')\n",
    "    plt.title('BER vs SNR')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(1e-5, 1e-0)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{filename_prefix}_BER_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "def generate_filename_prefix(batch_size, batch_num, epochs, lr, sample_num, seed):\n",
    "    base_dir = \"./models\"\n",
    "    subfolder_name = f\"28-12-lay2dim32-7035change_decoderbs{batch_size}_bn{batch_num}_ep{epochs}_lr{lr}_sm{sample_num}_seed{seed}/\"\n",
    "    subfolder_path = os.path.join(base_dir, subfolder_name)\n",
    "    \n",
    "    # 创建文件夹（如果不存在）\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "        # 确保路径末尾有 '/'\n",
    "    if not subfolder_path.endswith(os.sep):  # os.sep 是系统分隔符（'/' 或 '\\'）\n",
    "        subfolder_path += os.sep\n",
    "    \n",
    "    return subfolder_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TDL_dB = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    delay = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    train_SNR =15.0\n",
    "    batch_size = 800\n",
    "    batch_num = 300\n",
    "    lr = 1e-3\n",
    "    epochs = 1000\n",
    "    sample_num = 1\n",
    "    SNR_testrange = [20]\n",
    "    # 生成文件名前缀\n",
    "    filename_prefix = generate_filename_prefix(batch_size, batch_num, epochs, lr, sample_num, seed)\n",
    "    \n",
    "#      训练模型\n",
    "    train(train_SNR, batch_size, batch_num, lr, epochs, TDL_dB, delay, sample_num)\n",
    "    #     # 准备测试数据\n",
    "    test_data = torch.tensor(np.random.randint(0, 2, [1, 128])).float().to(device)\n",
    "    test_data = test_data.reshape(1, 1, -1)\n",
    "    h_r, h_i = H_sample(TDL_dB, delay, 1)\n",
    "    \n",
    "    # 测试最佳BER模型\n",
    "    # 测试最佳BER模型\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing Best BER Model\")\n",
    "    net_ber = Net()\n",
    "    net_ber.to(device)\n",
    "    net_ber.load_state_dict(torch.load(f'{filename_prefix}_best.pth', \n",
    "                                     map_location=lambda storage, loc: storage.cuda(0)))\n",
    "    net_ber.eval()\n",
    "    BER_ber = test_model(net_ber, SNR_testrange, filename_prefix)\n",
    "    print(\"BER results:\", BER_ber)\n",
    "\n",
    "    # 模型统计信息（以最佳BER模型为例）\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Model Summary (Best BER Model)\")\n",
    "    model_summary = summary(net_ber, input_data=(test_data, 15, h_r, h_i, 1), verbose=0)\n",
    "    print(model_summary)\n",
    "    \n",
    "    flops, params = thop.profile(net_ber, inputs=(test_data, 15, h_r, h_i, 1), verbose=False)\n",
    "    flops, params = thop.clever_format([flops, params], \"%.3f\")\n",
    "    print(\"thop-flops & trainable parameters:\", [flops, params])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
